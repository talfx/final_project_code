# -*- coding: utf-8 -*-
"""Final_Project_Data_Analysis_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yD9ZxXe4RIrijs5PxBVxh3f8_1cWM-Nq

# **Final Data Science Project**

## **Project Details**

**Subject:** Nature Reserve Sites In Israel.

---


**Target:** Predict Sites Entries, Present Recommendations and Information.

---

**Models:** Clustering, Neural Networks, Linear Regressions.



---


**Instructions:** Run all cells, and start the main section. models and output can be saved and uploaded on colab and then downloaded manually(after saving file with the interface, go to the left tab and download file, this file can later be uploaded and integrated on this project) to your PC for faster future predictions.
This project is made for the long run, any future data additions will work, although feature names should not be altered.


---


**Made by:** Tal Golan.


---

## Dictionaries Declaration
"""

global MLRs, NN, Clustering,Model_Index,Run_Check
MLRs={}
NNs={}
Clustering={}
Run_Check=0
Model_Index=None

"""## Imports"""

#Imports
from google.colab import files  
from pathlib import Path
import pickle
from threading import Timer
import pandas as pd
import numpy as np
import datetime as datetime
from sklearn.preprocessing import MinMaxScaler
from plotly.subplots import make_subplots
import plotly.graph_objects as go
from sklearn import metrics
from sklearn.metrics import r2_score
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from datetime import *
from statistics import mean
import datetime
import matplotlib.pyplot as plt
import math
from IPython.display import clear_output 
import sys, os
import keras
keras.__version__
from keras import models
from keras import layers
from keras import backend as K
from logging import warning
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import heapq
import json, requests, urllib, io
!pip install cryptography
from cryptography.fernet import Fernet
!pip install kneed
from kneed import KneeLocator
!matplotlib inline

"""## Uploading Data"""

# Getting data. 
global dfc # Clustering
global df # MLRs,NNs
global pao #personal token
def Main_Project():
  global df,pao,dfc_info
  user='talfx'
  pao='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'

  github_session = requests.Session()
  github_session.auth = (user, pao)

  # providing raw url to download csv from github
  csv_url1 = 'https://raw.githubusercontent.com/talfx/Final-Project/7d7e68b469e2fb8b8b99196df683c28329f43e23/df_Daily1.csv'
  csv_url2 = 'https://raw.githubusercontent.com/talfx/Final-Project/7d7e68b469e2fb8b8b99196df683c28329f43e23/df_Daily2.csv'
  csv_url3 = 'https://raw.githubusercontent.com/talfx/Final-Project/7d7e68b469e2fb8b8b99196df683c28329f43e23/df_info.csv'

  download1 = github_session.get(csv_url1).content
  downloaded_csv1 = pd.read_csv(io.StringIO(download1.decode('utf-8')))
  download2 = github_session.get(csv_url2).content
  downloaded_csv2 = pd.read_csv(io.StringIO(download2.decode('utf-8')))
  download3 = github_session.get(csv_url3).content
  downloaded_csv3 = pd.read_csv(io.StringIO(download3.decode('utf-8')))
  df=pd.concat([downloaded_csv1,downloaded_csv2]).reset_index()
  dfc_info=downloaded_csv3

"""## Global Data prep"""

def Change_Names_Spaces(Some_df,feature):
    Some_df[feature] = Some_df[feature].str.replace(' ', '_',regex=True)
    Some_df[feature] = Some_df[feature].str.replace('-', '_',regex=True)
    Some_df[feature] = Some_df[feature].str.replace("_–_", '_',regex=True)
    Some_df[feature] = Some_df[feature].str.replace('(', '',regex=True)
    Some_df[feature] = Some_df[feature].str.replace(')', '',regex=True)
    return Some_df
def Starting_Data_Prep_Main():
  global df,dfc,dfc_info
  
  df['Date']= pd.to_datetime(df['Date'],format="%Y/%m/%d")
  df=Change_Names_Spaces(df,'Site_Name')
  dfc_info=Change_Names_Spaces(dfc_info,'Site_Name')
  df['pm10'].fillna(value=df['pm10'].mean(), inplace=True)
  df['pm2.5'].fillna(value=df['pm2.5'].mean(), inplace=True)
  df['nox'].fillna(value=df['nox'].mean(), inplace=True)
  df['so2'].fillna(value=df['so2'].mean(), inplace=True)

        
pd.set_option('display.max_columns', None)
class bcolors:
    GREEN = '\033[92m' #GREEN
    YELLOW = '\033[93m' #YELLOW
    RED = '\033[91m' #RED
    RESET = '\033[0m' #RESET COLOR
    CYAN= '\033[96m' #CYAN 
    HEADER='\033[95m' #HEADER
    BLUE='\033[94m' #BLUE
    
def Orginize_OutPut(old_df):
  new_df=old_df.copy()
  arr=[]
  arr2=[old_df.columns[-3],old_df.columns[-2],old_df.columns[-1]]
  for item in arr2:
    arr.append(item)
  for item in new_df.columns[:-3]:
    arr.append(item)
  print(new_df[arr].round(2).head().to_markdown())

"""# **Clustering**

## Choosing Features
"""

#Choosing which features to cluster, taking means for features that change daily
def choice():
  global dfc5,dfc,dfc_info,df
  dfc5=df.copy()
  dfc_info=dfc_info.groupby('Site_Name').mean()
  dfc5=dfc5.groupby('Site_Name').mean()
  dfc5.drop(['index','Unnamed: 0','Unnamed: 0'], axis=1, inplace=True)
  dfc_info.drop('Unnamed: 0', axis=1, inplace=True)
  dfc=dfc5.join(dfc_info)
  dfc5=dfc.copy()

"""##Scaling"""

#Cluster scaling, saved seperately from MLRs and NNs
def ClusterScale():
  global scalerc,scaled_dfc,dfc
  scalerc = StandardScaler()
  scalerc.fit(dfc)
  scaled_dfc = pd.DataFrame(scalerc.transform(dfc), columns=dfc.columns)

"""## Elbow Test"""

#Elbow Test, choosing the best value visually
def ElbowTest():
  global scaled_dfc,sse
  sse = []
  if len(scaled_dfc)<3:
    return False
  k=3
  while 1:
    km = KMeans(n_clusters=k, random_state=1234)
    km.fit(scaled_dfc)
    if(km.inertia_==0):
      break
    k+=1
      
    sse.append(km.inertia_)
  
  fig = go.Figure()
  fig.add_trace(go.Scatter( y=sse, mode='markers+lines'))
  fig.update_layout(
      title="Elbow Test",
      xaxis_title="K",
      yaxis_title="Inertia",
      font=dict(
          family="Courier New, monospace",
          size=14,
          color="RebeccaPurple"
      ),
      width=800
  )

  fig.show()

"""## silhouette Test"""

#Closer to 1 --> the clusers are further apart
# (b - a) / max(a, b) 
#  b is the distance between a sample and the nearest other clusters, for each point

def SillTest():
  global dfc,scaled_dfc,sil
  sil = []
  k=2
  if len(dfc)<3:
    return False

  kmeans = KMeans(n_clusters=len(dfc)-1,random_state=1234).fit(scaled_dfc)
  labels = kmeans.labels_
  
  score3=silhouette_score(scaled_dfc, labels, metric = 'euclidean')
  score2=score3
  
   
  for i in range(0, len(dfc)-3) :
    kmeans = KMeans(n_clusters=k,random_state=1234).fit(scaled_dfc)
    labels = kmeans.labels_
    score1=silhouette_score(scaled_dfc, labels, metric = 'euclidean')
    
    if score1==score2 and score1==score3:
      break
    k+=1
    sil.append(score1)
    score2=sil[-1]
    if k>=4:
     score3=sil[-2]

  fig = go.Figure()
  fig.add_trace(go.Scatter(y=sil, mode='markers+lines'))
  fig.update_layout(
      title="Sillhouette Test",
      xaxis_title="K",
      yaxis_title="Inertia",

      font=dict(
          family="Courier New, monospace",
          size=14,
          color="RebeccaPurple"
      ),
      width=800
  )

  fig.show()

"""## Clustering Creation"""

#Starting Clustering and saving results in a list
def Cluster():
  global cluster_means,K_means,Clusters_List,labels,dfc,dfcc
  kmeans = KMeans(n_clusters=K_means, random_state=1234)
  kmeans.fit_predict(scaled_dfc)
  kmeans.cluster_centers_
  
  labels = kmeans.labels_
  
  dfc['cluster'] = labels
  
  cluster_means = dfc.groupby('cluster').mean() 
  
  cluster_centers = scalerc.inverse_transform(kmeans.cluster_centers_)
  
  dfcc=dfc.copy()
  dfcc['SiteName'] = dfc.index
  dfcc = dfcc.rename_axis('index1').reset_index()
  
  dfcc = pd.crosstab([dfcc.cluster, dfcc.SiteName], 0, margins=True)
  i=0
  
  Cluster_List=[]
  Clusters_List=[]
  
  for index, row in dfcc.iterrows():
    if index[0] is not 'All':
      if float(index[0])>i:
        i+=1
        Clusters_List.append(Cluster_List)
        Cluster_List=[]
      Cluster_List.append(index[1])
      
  Clusters_List.append(Cluster_List)
def Cluster_by():
  
  global cluster_means,K_means,Clusters_List,labels,dfc,scaled_dfc
  kmeans = KMeans(n_clusters=K_means, random_state=1234)
  kmeans.fit_predict(scaled_dfc)
  kmeans.cluster_centers_
  
  labels = kmeans.labels_
  
  dfc['cluster'] = labels
  
  cluster_means = dfc.groupby('cluster').mean() 
  
  cluster_centers = kmeans.cluster_centers_
  dfc['Target'] = dfc.index
  dfc = pd.crosstab([dfc.cluster, dfc.Target], 0, margins=True)
  Cluster_List=[]
  Clusters_List=[]
  i=0
  for index, row in dfc.iterrows():
    if index[0] is not 'All':
      if float(index[0])>i:
        i+=1
        Clusters_List.append(Cluster_List)
        Cluster_List=[]
      Cluster_List.append(index[1])
      
  Clusters_List.append(Cluster_List)

"""## Clustering Details"""

# Clustering Details

def Clustering_Model_Details():
  global Clusters_List,cluster_means,labels,K_means,dfc,cluster_stds
  count=dfc['cluster'].value_counts()
  cluster_stds = dfc.groupby('cluster').std() 
  cluster_stds.fillna(0, axis=1,inplace=True)
  
  print("----------------Sites Clustered---------------- \n")
  for i in Clusters_List:
    print(i)
  print("----------------K-means---------------- \n",K_means)
  print("----------------Cluster Labels---------------- \n",labels)
  print("----------------Cluster Count---------------- \n",count)
  print("----------------Cluster Means---------------- \n",cluster_means.round(2).to_markdown())
  print("----------------Cluster STD---------------- \n",cluster_stds.round(2).to_markdown())

"""## Find Sites Name in Clusters"""

#Printing Recommendations: showing sites within the same cluster\, and highligting which features mostly correlated

def FindCluster(Name):
  global Clusters_List
  loc=0
  c=0
  arr=[]
  for i in range(0,len(Clusters_List)):
    
    for j in range(0,len(Clusters_List[i])):
      if Clusters_List[i][j]==Name:
        loc=i
        c=1
        break
    if c==1:
      break

  for i in range(0,len(Clusters_List[loc])):
    if Clusters_List[loc][i]!=Name:
      arr.append(Clusters_List[loc][i])
  
  return arr
  
def Print_Cluster_Info(arr,Name):
  name_df=dfc.loc[Name]
  print("--------------Our Recommendations:--------------\n")
  print("------------Here are other sites you might be interested in! (Similar attributes are marked)------------\n")
  for item in arr:
    if item!=Name:
      print("\nSite Name: ",item )
      Group_Item_df=dfc.loc[item]
      for column in dfc.columns:
        if ((name_df[column]==Group_Item_df[column] and Group_Item_df[column]==1) or\
            ((column=='Tourists_Count' or\
              column=='Israelis_Count' or column=='Total') and \
             (abs(name_df[column]-Group_Item_df[column]<20))  ) or\
            (column=='Temperature' and abs(name_df[column]-Group_Item_df[column]<5))) and column !='cluster' :
          print(bcolors.GREEN," ",column,": ", round(Group_Item_df[column],2), bcolors.RESET,end='')

  print("\n----------------------------------------------------------------------\n")

"""## Finding Optimal K-Means"""

#Finding an optimal K-Means
#plot 
def calc_distance_Point_To_Line(x1, y1, a, b, c):
  d = abs((a * x1 + b * y1 + c)) / (math.sqrt(a * a + b * b))
  return d
#Calculated by creating a line and therfore finding the best K by the ElbowTest
#Then taking %5 error rate to the left and right, and using the Sillhouette test to find the highest K-
#within that error rate, therfore finding the best K mathematically.
def Optimal_K():
  global sse,K_Means_All,sil
  K_Means_All=range(3,len(scaled_dfc))
  a = sse[0] - sse[-1]
  b = K_Means_All[-1] - K_Means_All[0]
  c1 = K_Means_All[0] * sse[-1]
  c2 = K_Means_All[-1] * sse[0]
  c = c1 - c2
  distance_of_points_from_line = []
  for k in range(len(K_Means_All)):
    distance_of_points_from_line.append(calc_distance_Point_To_Line(K_Means_All[k], sse[k], a, b, c))
  OptimalElbow=distance_of_points_from_line.index(max(distance_of_points_from_line))
  ErrorRate=int(len(scaled_dfc)*0.05)+1
  OptimalSil= sil.index(max(sil[OptimalElbow-ErrorRate:OptimalElbow+ErrorRate]))
  return OptimalSil,ErrorRate,OptimalElbow
def K_Evaluation():
  #Plot
  global sse,K_Means_ALG,sil
  K_Means_ALG=Optimal_K()
  
  fig = go.Figure()
  fig.add_trace(go.Scatter( y=sse, mode='markers+lines',name="Inertia by K"))
  fig.add_vline(x=K_Means_ALG[2],name="Longest Distance",line_color='#00ff00')
  fig.add_trace(go.Scatter( y=[sse[0],sse[-1]],x=[0,len(sse)-1], mode='markers+lines',name="Line MIN to MAX"))
  fig.add_annotation(dict(font=dict(color='blue',size=15),
                                        x=0,
                                        y=-0.12,
                                        showarrow=False,
                                        text="Optimal K By Distance= "+str(K_Means_ALG[2]),
                                        textangle=0,
                                        xanchor='left',
                                        xref="paper",
                                        yref="paper"))

  fig.update_layout(
      title="Elbow Test",
      xaxis_title="K",
      yaxis_title="Inertia",
      font=dict(
          family="Courier New, monospace",
          size=14,
          color="RebeccaPurple"
      ),
      width=800
  )

  fig.show()
  fig = go.Figure()
  fig.add_trace(go.Scatter( y=sil, mode='markers+lines',name="Inertia by K"))
  fig.add_vline(x=K_Means_ALG[2]-K_Means_ALG[1],line_dash="dash")
  fig.add_vline(x=K_Means_ALG[0],line_color="#00ff00")
  fig.add_vline(x=K_Means_ALG[2],line_color="#ff4040")
  fig.add_vline(x=K_Means_ALG[2]+K_Means_ALG[1],line_dash="dash")
  fig.add_annotation(dict(font=dict(color='blue',size=15),
                                        x=0,
                                        y=-0.12,
                                        showarrow=False,
                                        text="Final Optimal K= "+str(K_Means_ALG[0]),
                                        textangle=0,
                                        xanchor='left',
                                        xref="paper",
                                        yref="paper"))


  fig.update_layout(
      title="Sillhouette Test",
      xaxis_title="K",
      yaxis_title="Inertia",
      font=dict(
          family="Courier New, monospace",
          size=14,
          color="RebeccaPurple"
      ),
      width=800
  )

  fig.show()
  return K_Means_ALG[0]
  
def K_Evaluation_No_Output():
  ElbowTest()
  SillTest()
  x= K_Evaluation()
  clear_output()
  return x

"""#**Overall Site's Info**

## Month names prep
"""

def Return_Months(list_of_Num):
  arr=[]
  list_of_Month_Names=["January","February","March","April","May","June","July","August","September","October","November","December"]
  for item in list_of_Num:
    arr.append(list_of_Month_Names[item-1])
  return arr

"""## Max and Min stats"""

def Site_Find_Info(column,Site_Name):
  df_temp=df.copy()
  df_temp=df_temp.loc[df_temp['Site_Name'] == Site_Name]
  if len(df_temp)<1:
    return
  max=df_temp[column].iloc[0]
  min=df_temp[column].iloc[0]
  min_index=0
  Max_index=0
  for i in range(len(df_temp[column])):
    if max< df_temp[column].iloc[i]:
      max=df_temp[column].iloc[i]
      Max_index=i
    if min > df_temp[column].iloc[i]:
      min=df_temp[column].iloc[i]
      min_index=i
  ListMinMAX=[]
  df_temp['Year'] = df_temp.apply(lambda x: x.Date.year, axis=1)
  df_temp['Month'] = df_temp.apply(lambda x: x.Date.month, axis=1)
  for i in range(1,13):
    count=0
    amount_counted=1
    
    df_temp2=df_temp.loc[(df_temp['Month'] == i) & (df_temp['Year']< 2020)]
    for j in range(len(df_temp2.index)):
      amount_counted+=1
      count+=df_temp2[column].iloc[j]
    ListMinMAX.append((count/amount_counted,i))
  heapq.heapify(ListMinMAX)
  Final_Max=heapq.nlargest(3, ListMinMAX)
  Final_Min=heapq.nsmallest(3, ListMinMAX)
  print("-------------------------Max and Min Stats of", column,"Entries-------------------------")
  print("Max ", column," Value:", max,"-------Date: [",df_temp["Date"].iloc[Max_index]\
        ,"]\nMin ", column," Value:", min,"-------Date: [",df_temp["Date"].iloc[min_index],"]" )
  print("Top 3 months by entry of",column,": ",Return_Months([items[1] for items in Final_Max])\
        ,"\nBottom 3 months by entry of",column,": ",Return_Months([items[1] for items in Final_Min]))
  print("---------------------------------------------------------------------------------------")

"""## Printing Overall Stats"""

def Site_Info(Name):
  clear_output()
  global dfc,df
  dfc11=dfc.copy()
  
  name_df=dfc11.loc[Name]
  print("--------------Site Info:--------------")
  print(bcolors.GREEN,"Site Name: ",Name,bcolors.RESET)
  count=0
  for column in dfc.columns:
    count+=1
    if column !='cluster':
      print(" ",column,": ", round(name_df[column],2),end='')
    if count == 3:
      count=0
      print("")

  Print_Cluster_Info(FindCluster(Name),Name)
  
  
  print("--------------Entries Stats--------------\n")
  Site_Find_Info("Total",Name)
  Site_Find_Info("Israelis_Count",Name)
  Site_Find_Info("Tourists_Count",Name)


  print("----------------------------------------------------------------------\n")

"""## Specific Date Stats"""

def Seperate_Site(Name,date):
  global df5
  dfc11=df5.copy()
  dfc11=dfc11.loc[dfc11['Site_Name'] == Name]
  dfc11=dfc11.loc[dfc11['Date'] == date]
  Arr=['Total','Israelis_Count','Tourists_Count','Temperature','is_HeatWave','Is_Summer','is_weekend','pm2.5','nox','so2','pm10']
  dfc11=dfc11[Arr]
  return dfc11

def Search_Site_By_Date(Name,date):
  print("------------------")
  dfc11=Seperate_Site(Name,date)
  print("Site Stats in date: %s"%(date))
  for i in range(len(dfc11.columns)):
    print("[%s: %s]"%(dfc11.columns[i],dfc11.iloc[0][i] if dfc11.iloc[0][i] % 1 == 0\
                      else round(dfc11.iloc[0][i],2)),end='')
  print("------------------")

def Enter_Date(Name):
  today=np.datetime64('today')
  while(True):
    clear_output()
    year=int(input("Enter Year: "))
    month=int(input("Enter Month: "))
    day=int(input("Enter day: "))
    try:
      answer=None
      c=1
      
      
      dfc11=Seperate_Site(Name,np.datetime64(datetime.date(year,month, day)))
      
      if dfc11.empty==True or today<np.datetime64(datetime.date(year,month, day)):
        c=2
        print("wrong entry \ no data for this date\n")
        print(bcolors.GREEN, "  Y",bcolors.RESET," to try again\n", bcolors.GREEN,"ANY",bcolors.RESET,"Input to go back")
        print(bcolors.GREEN,"\n Enter Command:",bcolors.RESET)
        answer=input(" ")
    except Exception as e:
      c=2
      print(e)
      print(bcolors.GREEN, "\n  Y",bcolors.RESET," to try again\n", bcolors.GREEN,"ANY",bcolors.RESET,"Input to go back")
      print(bcolors.GREEN,"\n Enter Command:",bcolors.RESET)
      answer=input(" ")

    if answer in ['Y','y']:
      continue
    if c==1:
      Site_Info(Name)
      Search_Site_By_Date(Name,np.datetime64(datetime.date(year,month, day)))
      return True
    return False

"""# Data Prep

## Date
"""

#Data prep

def Summer (row,y,min):
  for i in range(0,y):
      if (row['Date'] >=pd.Timestamp(date(min+i, 6, 1)) and row['Date'] < pd.Timestamp(date(min+i, 9, 1))):
        return 1  
  return 0
def Autumn (row,y,min):
  for i in range(0,y):
      if (row['Date'] >=pd.Timestamp(date(min+i, 9, 1)) and row['Date'] < pd.Timestamp(date(min+i, 12, 1))):
        return 1  
  return 0
def Spring (row,y,min):
  for i in range(0,y):
      if (row['Date'] >=pd.Timestamp(date(min+i, 4, 1)) and row['Date'] < pd.Timestamp(date(min+i, 6, 1))):
        return 1  
  return 0
def Winter (row,y,min):
  for i in range(0,y):
      if (row['Date'] >=pd.Timestamp(date(min+i, 12, 1)) and row['Date'] <= pd.Timestamp(date(min+i, 12, 31))) |\
          (row['Date'] >=pd.Timestamp(date(min+i, 1, 1)) and row['Date'] < pd.Timestamp(date(min+i, 4, 1))):
        return 1  
  return 0
def check(row,y,min):
  if Summer(row,y,min):
    return 3
  if Winter(row,y,min):
    return 1
  if Spring(row,y,min):
    return 2
  if Autumn(row,y,min):
    return 4
  return 0
def Date_Data_prep():
  global df5
  df5 = df.copy()
  df5['Year'] = df.apply(lambda x: x.Date.year, axis=1)
  y=(df5['Year'].max()+1)-2016
  min=df5['Year'].min()
  df5['Is_Summer']=df5.apply (lambda row: Summer(row,y,min), axis=1)
  df5.drop(['index','Unnamed: 0'], axis=1, inplace=True)
# df5['Season']=df5.apply (lambda row: check(row,y,min), axis=1)

"""## Choosing Site, prediction and Pollution Features"""

#Choosing Site , options: Prediction, Feature select
def SiteEx(SiteName,b,pred,Year_List):
  global df2, predict
  predict=pred
  df2=df5.copy()
  if Year_List is not False:
    df2=df2.loc[(df2['Site_Name']==SiteName) & (df2['Year'].isin(Year_List))]
  else:
    df2=df2.loc[df2['Site_Name']==SiteName]

  if b== True:
    df2=df2[[pred,'Temperature','is_HeatWave','Is_Summer','is_weekend','pm2.5','nox','so2','pm10']]
  else:
    df2=df2[['Temperature',pred,'Is_Summer','is_HeatWave','is_weekend']]

"""## Splitting to X,y"""

#y=Target feature, X=independent features
def XnY(): 
  global predict, df2,y,X
  y=df2[predict]
  X=df2.drop(columns=[predict],axis=1)

"""## Plotting Features"""

#Plotting features
def PlotFeatures():
  global X,y
  print("--------------------Site Features--------------------\n")
  rows=math.ceil( len(X.columns)/3)
  cols = 3
  fig = make_subplots(rows=rows, cols=cols, shared_yaxes=True )
  count=0
  for i in range(rows):
    for j in range(cols):
      if count < len(X.columns):
        fig.add_trace(go.Scatter(
        y=y, 
        x=X.iloc[:,count],
        mode='markers',
        marker_color='blue',
        marker_size=2.5,
        name=X.columns[count]
        ),row=i+1, col=j+1
      )
        
        fig.update_xaxes(title_text=X.columns[count], row=i+1, col=j+1)
        count+=1
      
    
  fig.update_layout(height=700, width=900,
                  title_text=""+predict+"Entries By Features")
  fig.show()
  print("\n")

"""## Train and Test"""

#Splitting Data to train and test
def split():
  global X_train,X_test,y_train,y_test,train_df,test_df
  X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1357)
  train_df = pd.merge(left=X_train, right=y_train, left_index=True, right_index=True)
  test_df = pd.merge(left=X_test, right=y_test, left_index=True, right_index=True)

"""## Scaling"""

#Scaling Test and Train models
def scale():
  global x_train_scaler, x_test_scaler,y_train_scaler,y_test_scaler,X_train_scaled,X_test_scaled,y_train_scaled,y_test_scaled

  x_train_scaler = StandardScaler()
  x_test_scaler = StandardScaler()
  y_train_scaler = StandardScaler()
  y_test_scaler = StandardScaler()

  X_train_scaled = x_train_scaler.fit_transform(X_train)
  X_test_scaled = x_test_scaler.fit_transform(X_test)
  y_train_scaled = y_train_scaler.fit_transform(pd.DataFrame(y_train))
  y_test_scaled = y_test_scaler.fit_transform(pd.DataFrame(y_test))

"""# **Neural Network**

## Model Structure
"""

# Building model
def build_model():
  global X_train_scaled,model
  model = models.Sequential()
  model.add(layers.Dense(64, activation='relu',
                          input_shape=(X_train_scaled.shape[1],)))
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(1))
  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
  return model
  
def Generic_build_model(lay,act,ls,met):
  global X_train_scaled,model
  model = models.Sequential()
  model.add(layers.Dense(lay[0], activation=act[0],
                          input_shape=(X_train_scaled.shape[1],)))
  for i in range(1,len(lay)+1):
    model.add(layers.Dense(lay[i], activation=act[i]))
  model.add(layers.Dense(1))
  model.compile(optimizer='rmsprop', loss=ls, metrics=[met])
  return model

"""## Train Network"""

#NN on Train Data set With validation, MSE as loss function, MAE as Epochs Metric

def TrainNN():
  global num_val_samples, model, history,X_train_scaled,y_train_scaled
  
  K.clear_session()
  num_val_samples = len(X_train_scaled) // 4
  num_epochs = 10
  all_mae_histories = []


  val_data = X_train_scaled[0: num_val_samples]
  val_targets = y_train_scaled[0: num_val_samples]


  partial_train_data = np.concatenate(
      [X_train_scaled[:0],
        X_train_scaled[num_val_samples:]],
      axis=0)
  partial_train_targets = np.concatenate(
      [y_train_scaled[:0],
        y_train_scaled[num_val_samples:]],
      axis=0)


  model = build_model()

  history = model.fit(partial_train_data, partial_train_targets,
                      validation_data=(val_data, val_targets),
                      epochs=num_epochs, batch_size=1, verbose=0)
  mae_history = history.history['val_mae']

"""## Plotting MAE and Loss"""

#Plotting NN MAE per epoch

def PlotMAE():
  print("---------------------MAE---------------------")
  global history
  history_dict = history.history
  loss_values = history_dict["mae"]
  val_loss_values = history_dict["val_mae"]
  epochs = range(1, len(loss_values) + 1)
  plt.plot(epochs, loss_values, "bo", label="Training mae")
  plt.plot(epochs, val_loss_values, "b", label="Validation mae")
  plt.title("Training and validation mae")
  plt.xlabel("Epochs")
  plt.ylabel("mae")
  plt.legend()
  plt.show()
  print("")
  

#Plotting NN loss per epoch
def PlotLoss():
  print("---------------------Loss------------------------")
  global history
  history_dict = history.history
  loss_values = history_dict["loss"]
  val_loss_values = history_dict["val_loss"]
  epochs = range(1, len(loss_values) + 1)
  plt.plot(epochs, loss_values, "bo", label="Training loss")
  plt.plot(epochs, val_loss_values, "b", label="Validation loss")
  plt.title("Training and validation loss")
  plt.xlabel("Epochs")
  plt.ylabel("Loss")
  plt.legend()
  plt.show()
  print("")

"""## Building Final Model"""

def BuildFinalNN():
  import sys
  import os
  global model,X_test_scaled,y_test_scaled

  # old_stdout = sys.stdout # backup current stdout
  # sys.stdout = open(os.devnull, "w")

  model.fit(X_train_scaled, y_train_scaled, epochs=80, batch_size=512)
  Test_Mse,Test_Mae = model.evaluate(X_test_scaled, y_test_scaled)
  print("Test MSE: ",round(Test_Mae,2),"\nTest MAE: ",round(Test_Mse,2))


  # sys.stdout = old_stdout # reset old stdout


def GetBuild():
  global model,X_test_scaled,y_test_scaled
  Test_Mse,Test_Mae = model.evaluate(X_test_scaled, y_test_scaled)
  print("Test MSE: ",round(Test_Mae,2),"\nTest MAE: ",round(Test_Mse,2))

"""# **Linear Regression**

## Building model
"""

#Starting MLR or calling existing MLR
def MLR():
  global lm
  lm = LinearRegression() 
  lm.fit(X_train_scaled,y_train_scaled)    
  coeff = lm.coef_[0]
  intercept = lm.intercept_[0] 
  print("\n\n-----------------REGRESSION EQUATION-----------------\n")        
  print('Coefficients: \n', "coeff =", coeff , ",  Intercept=",intercept,"\n" )
  print("The regression equation is:" ,"\nEntries=\n" ,intercept)
  for i in range(0,len(coeff)):
    print( "+ ",round(coeff[i],2)," * ",X.columns[i])


def getMLR():
  global lm
  coeff = lm.coef_[0]
  intercept = lm.intercept_[0]  
  print("\n\n-----------------REGRESSION EQUATION-----------------\n")         
  print('Coefficients: \n', "coeff =", coeff , ",  Intercept=",intercept,"\n" )
  print("The regression equation is:" ,"\nEntries=\n" ,intercept)
  for i in range(0,len(coeff)):
    print( "+ ",round(coeff[i],2)," * ",X.columns[i])

"""## Training Model"""

# Training the model and printing results
def TrainPredict():
  global train_df,predicted_train,X_train_scaled,y_train_scaler,y_train
  fitted_scaled = lm.predict(X_train_scaled) 
  fitted = y_train_scaler.inverse_transform(fitted_scaled)
  predicted_train = round(pd.Series(fitted[:,0], index=y_train.index, name='Predicted_train'),ndigits=2)
  train_df = pd.merge(left=train_df, right=predicted_train , left_index=True, right_index=True)
  

  print("\n-----------------TRAIN MODEL-----------------\n")
  Orginize_OutPut(train_df)

"""## Testing Model"""

# Testing the model and printing results
def TestPredict():
  global test_df,predicted_test,X_test_scaled,y_test_scaler,y_test
  fitted_scaled = lm.predict(X_test_scaled)
  fitted = y_test_scaler.inverse_transform(fitted_scaled)
  predicted_test = round(pd.Series(fitted[:,0], index=y_test.index, name='Predicted_test'),ndigits=2)
  test_df = pd.merge(left=test_df, right=predicted_test, left_index=True, right_index=True)
  print("\n-----------------TEST MODEL-----------------\n")
  Orginize_OutPut(test_df)

"""## Plotting Residuals"""

# Printing How far the prediction is in the Train model and the Test model from the real value
def PlotR():
  global train_df,test_df
  train_df['residuals'] = train_df.Predicted_train - train_df[predict]
  test_df['residuals'] = test_df.Predicted_test - test_df[predict]
  
  fig= go.Figure()
  fig.add_trace(
      go.Scatter(
          x=train_df.Predicted_train,
          y=train_df.residuals,
          mode='markers',
          name='train residuals',
          marker_color='blue',
          marker_size=1.5,
          marker_line_width=0,
      )
  )
  fig.add_trace(
    go.Scatter(
        x=test_df.Predicted_test,
        y=test_df.residuals,
        mode='markers',
        name='test residuals',
        marker_color='red',
        marker_size=1.5,
        marker_line_width=0,
    )
  )
  fig.add_trace(
    go.Scatter(
        x=test_df.Predicted_test,
        y=test_df.residuals*0,
        mode='lines',
        name='zero line',
        marker_color='black',
        marker_size=1.5,
        marker_line_width=0,
    )
  )
  fig.update_layout(
      title="Residuals of Predicted Entries",
      xaxis_title="Predicted Entries",
      yaxis_title="Residuals",
      font=dict(
          size=14,
          color="RebeccaPurple"
      )
  )
  fig.show()
  print("\n")

"""## Regression Stats"""

#Cost Function and R2
def TrainCost():
  global predict
  print("------ TRAIN DATA ------")
  print("MSE:",round(metrics.mean_squared_error(train_df[predict], train_df.Predicted_train),2))
  print("RMSE:",round(np.sqrt(metrics.mean_squared_error(train_df[predict], train_df.Predicted_train)),2))
  print("MAE:",round(metrics.mean_absolute_error(train_df[predict], train_df.Predicted_train),2),"\n")
def TestCost():
  global predict
  print("------ TEST DATA ------")
  print("MSE:",round(metrics.mean_squared_error(test_df[predict], test_df.Predicted_test),2))
  print("RMSE:",round(np.sqrt(metrics.mean_squared_error(test_df[predict], test_df.Predicted_test)),2))
  print("MAE:",round(metrics.mean_absolute_error(test_df[predict], test_df.Predicted_test),2),"\n") 
def R2():
  global predict
  print("------ R2 Score ------ \n", round(r2_score(train_df[predict], train_df.Predicted_train),2),"\n")
def STD():
  # Based on STD formula ((∑(Y−Yp)^2) /(n-2)) ^0.5 
  # https://www.investopedia.com/terms/r/residual-standard-deviation.asp
  global predict,train_df,test_df
  def STD_Each(df,S):
    count=0
    if S=='Test':
      Name="Predicted_test"
    if S=='Train':
      Name="Predicted_train"
      
    for index, row in df.iterrows():
      count+=1
      sum=pow((row[Name]-row[predict]),2)
    if count>2:
      STD_R=pow((sum/(count-2)),0.5)
      print("------",S,"STD Error ------\n", round(STD_R,2),"\n")
  STD_Each(train_df,"Train")
  STD_Each(test_df,"Test")

"""# **Backend**

## Save,use or replace models
"""

#saving, getting and using MLRs

def save(SiteName,Pol,Predict_Choice,years,date,Model_type):
  global test_df,train_df,lm,X,y,predict,model,history,x_train_scaler,y_test_scaled,X_test_scaled
  array1=[SiteName,Pol,Predict_Choice,date,years]
  array2=[]
  if Model_type=='MLR':
    array2=[test_df,train_df,X,y,lm,x_train_scaler]
  if Model_type =='NN':
    array2=[test_df,train_df,X,y,model,history,x_train_scaler,X_test_scaled,y_test_scaled]
  return [array1,array2]

def replace(k, Store2,Model_type): 
  global test_df,train_df,lm,X,y,predict,model,history,x_train_scaler,y_test_scaled,X_test_scaled,Model_Index
  Store=Store2.copy()
  test_df = Store[k][1][0]
  train_df=Store[k][1][1]
  predict=Store[k][0][2] 
  X=Store[k][1][2]
  y=Store[k][1][3]
  Model_Index=k

  if Model_type=='MLR':
    lm=Store[k][1][4]
    x_train_scaler=Store[k][1][5]
    

    
  if Model_type =='NN':
    model=Store[k][1][4]
    history=Store[k][1][5]
    x_train_scaler=Store[k][1][6]
    X_test_scaled=Store[k][1][7]
    y_test_scaled=Store[k][1][8]
    
def use(Model_type):
  global test_df,train_df,Model_Index
  PlotFeatures()
  key=Model_Index
  if Model_type=='MLR':
    
    print("\n-----------------TRAIN MODEL-----------------\n")
    Orginize_OutPut(train_df)
    print("\n-----------------TEST MODEL-----------------\n")
    Orginize_OutPut(test_df)
    getMLR()
    PlotR()
    TrainCost()
    TestCost()
    R2()
    STD()
  if Model_type =='NN':
    PlotMAE()
    PlotLoss()
    GetBuild()

"""## **Running Clustering functions**"""

def Cluster_Run(): 
  ClusterScale()
  Cluster()
  global Clustering, Clusters_List,cluster_means,labels,K_means
  Clustering={'Clusters_List':Clusters_List,'cluster_means':cluster_means,'cluster_labels':labels, 'K_means':K_means}

  
def P_Clusters():
  clear_output()
  print("-------------------------Clustering Stats-------------------------\n")
  ElbowTest()
  SillTest()
  Clustering_Model_Details()
  print("--------------------------------------------------\n")

def ClusterChooseK(K):
  global K_means
  K_means=K
  Cluster_Run()

def ClusterReplace():
  global Clustering, Clusters_List,cluster_means,labels,K_means
  K_means=Clustering["K_means"]
  cluster_means=Clustering["cluster_means"]
  Clusters_List=Clustering["Clusters_List"]
  labels=Clustering["cluster_labels"]

def ClusterFind(Name):
  Print_Cluster_Info(FindCluster(Name),Name)

"""## **Starting models, saving locally**"""

#Calling Functions and saving to MLRs
def start(SiteName,Pol,Predict_Choice,years,Model_type):
  SiteEx(SiteName,Pol,Predict_Choice,years)
  XnY()
  
  split()
  scale()
  if Model_type=='MLR':
    PlotFeatures()
    MLR()
    TrainPredict()
    TestPredict()
    PlotR()
    TrainCost()
    TestCost()
    R2()
    STD()

  if Model_type=='NN':
    build_model()
    TrainNN()
    BuildFinalNN()
    clear_output()
    print("\n\n >>>>>>>>>>>>  NN Details  <<<<<<<<<<<< \n\nSite: %s| Prediction By: %s | Pollution Variables: %s\n" %(SiteName,Predict_Choice,Pol))
    PlotFeatures()
    GetBuild()
    PlotMAE()
    PlotLoss()
    

#Checking if MLR was already done with the same parameters
def StartAndSave(SiteName,Pol,Predict_Choice,years,Model_type):
  global MLRs,NNs,Model_Index

  ins=0
  if Model_type=='MLR':
    for key in MLRs:
      if MLRs[key][0][0]==SiteName and MLRs[key][0][1]==Pol and MLRs[key][0][2]==Predict_Choice and MLRs[key][0][4]== years:
        print("Linear Regression based on this info was already built at %s!" %(MLRs[key][0][3]))
        check = input("\nTo Use the current MLR press Y, to overwrite the current MLR press X")
        if check in [True,1,'Y','y']:
          replace(key,MLRs,"MLR")
          use("MLR")
        else:
          start(SiteName,Pol,Predict_Choice,years,"MLR")
          MLRs.update({key:save(SiteName,Pol,Predict_Choice,years,date.today().strftime("%d/%m/%Y"),'MLR')})
        ins=1
      
    if ins==0:
      start(SiteName,Pol,Predict_Choice,years,'MLR')
      MLRs.update({str(len(MLRs)+1):save(SiteName,Pol,Predict_Choice,years,date.today().strftime("%d/%m/%Y"),'MLR')})
      Model_Index=str(len(MLRs))


  if Model_type=='NN':
    for key in NNs:
      if NNs[key][0][0]==SiteName and NNs[key][0][1]==Pol and NNs[key][0][2]==Predict_Choice and NNs[key][0][4]== years:
        print("Neural Network based on this info was already built at %s!" %(NNs[key][0][3]))
        check = input("\nTo Use the current Neural Network press Y, to overwrite the current Neural Network press X")
        if check in [True,1,'Y','y']:
          replace(key,NNs,"NN")
          use("NN")
        else:
          start(SiteName,Pol,Predict_Choice,years,'NN')
          NNs.update({key:save(SiteName,Pol,Predict_Choice,years,date.today().strftime("%d/%m/%Y"),'NN')})
        ins=1
      
    if ins==0:
      start(SiteName,Pol,Predict_Choice,years,'NN')
      NNs.update({str(len(NNs)+1):save(SiteName,Pol,Predict_Choice,years,date.today().strftime("%d/%m/%Y"),'NN')})
      Model_Index=str(len(NNs))

"""## **Cryptography**

### **Get Key**
"""

def Get_Key():
  user='talfx'
  pao='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'

  github_session = requests.Session()
  github_session.auth = (user, pao)

  # providing raw url to download csv from github
  csv_url1 = 'https://raw.githubusercontent.com/talfx/Final-Project/a33b36c4e4b4ba28b72cdf55fc2ce26102950523/filekey.key'

  return github_session.get(csv_url1).content

"""### **Encrypt Save File**"""

def Encrypt_Save_File():
  with open('Models.pkl', 'rb') as file:
      original = file.read()
  fernet = Fernet(Get_Key())       
  encrypted = fernet.encrypt(original)
  os.remove('Models.pkl')
  with open('Models.pkl', 'wb') as encrypted_file:
    encrypted_file.write(encrypted)
  return True

"""### **Decrypt Save File**"""

def Decrypt_Save_File(Uploaded_Name):
  fernet = Fernet(Get_Key())
  with open(Uploaded_Name, 'rb') as enc_file:
    encrypted = enc_file.read()
  os.remove(Uploaded_Name)
  decrypted = fernet.decrypt(encrypted)
  with open(Uploaded_Name, 'wb') as dec_file:
      dec_file.write(decrypted)

"""### **Saving Models**"""

def Save_All():
  global MLRs, NNs,Clustering,pao
  Models=[MLRs,NNs,Clustering,pao]
  with open('Models.pkl', 'wb') as f:
    pickle.dump(Models, f)
  Encrypt_Save_File()

"""### **Uploading Models**"""

def Upload_All():
  global pao,MLRs,NNs,Clustering
  try:
    c=0
    print("\n--------------Upload a Save---------------\n\n")
    my_file = Path("Models.pkl")
    if my_file.is_file():
      os.remove("Models.pkl")
    up = files.upload()
    Uploaded_Name=list(up.keys())[0]
    Decrypt_Save_File(Uploaded_Name)
    with open(Uploaded_Name, 'rb') as f: 
      
      loaded_dict = pickle.load(f)
      if loaded_dict[3] != pao:
        os.remove(Uploaded_Name)
        return False
      os.rename(Uploaded_Name, "Models.pkl")
      MLRs=loaded_dict[0]
      NNs=loaded_dict[1]
      Clustering=loaded_dict[2]
      ClusterReplace()
      Encrypt_Save_File()
      if Encrypt_Save_File():
        return True
      return False
  
  except Exception as e:
    try:
      if Path(Uploaded_Name).is_file():
        os.remove(Uploaded_Name)
    except Exception as e:
      return e,False
    return e,False

"""# **Frontend**

## Prediction Entry
"""

#Get Input From The User For Prediction
def PredInput():
  clear_output()
  print("---------Enter Inputs---------")
  Feature=[]
  for i in X.columns:
    
    if(i!='Season'):
      s=float(input("\n  Enter %s " %(i)))
      
    else:
      s= float(input("\n  Enter Month "))
      if (s<13 and s >0):
        if (s >= 6 and s <  9):
          s=3 
        if (s >= 9 and s <  11):
          s=4  
        if (s >=11 and s <=  12) or (s<4 and s>0):
          s=1
        if (s >= 4 and s < 6):
          s=2
    
    while not (s<50 and s>-20) and i=='Temperature'  :
      clear_output()
      print("  Wrong Entry: must be between -20 and 50")
      s= float(input("\n  Enter %s " %(i)))

    while not (s<13 and s >0) and i=='Season' :
      clear_output()
      print("  Wrong Entry: Must be between 1-12")
      s= float(input("\n  Enter Month "))
      if (s<13 and s >0):
        if (s >= 6 and s <  9):
          s=3 
        if (s >= 9 and s <  11):
          s=4  
        if (s >=11 and s <=  12) or (s<4 and s>0):
          s=1
        if (s >= 4 and s < 6):
          s=2
        
    while (s!=1 and s!=0) and( i=='is_weekend' or i=='is_HeatWave' or i=='Is_Summer'):
      clear_output()
      print("  Wrong Entry: Must be 0 or 1")
      s= float(input("  Enter %s " %(i)))
     
    Feature.append(s)

  return Feature

"""## **Predicting**"""

#Predicting based on a new entry
def PredictX(arr,Model_type):
  clear_output()
  use(Model_type)
  global dfPred,model,lm
  dfpred = pd.DataFrame([arr], columns=list(X.columns))
  NewXpred=None
  NewX_Scaled = x_train_scaler.inverse_transform(dfpred)
  if Model_type=='NN':
    NewXpred = model.predict(NewX_Scaled)
    print("---------------Predicting With Neural Network-----------")
  if Model_type=='MLR':
    NewXpred = lm.predict(NewX_Scaled)
    print("---------------Predicting With Multiply Linear Regression-----------")

  print("---------------Predicting By----------- \n"+dfpred.to_string(index=False))
  if(NewXpred[0][0]<0):
    print("\n---------------Predicted value---------------\n",0)
  else:
    print("---------------Predicted value---------------\n",round(NewXpred[0][0]//1))

"""## **Print Functions**"""

#Printing Functions

def Hello():
  print("------------------------------",bcolors.CYAN,"Start",bcolors.RESET,\
        "------------------------------------\n")
  print("Welcome To The Main Project!\n")
  print("Title: Nature Reserves in Israel\n")
  print("Target: Predictions, Recommendations and Presenting Data\n")
  print("Project Made by Tal Golan\n")
  

def initial():
  print("------------------------------",bcolors.CYAN,"Lets Start!",bcolors.RESET,\
        "------------------------------------\n")
  print("Welcome!\n")
  print("Title: Data Analysis\n")
  print("Models: Learning Your Data And Creating Linear Regression, Neural Network, and Clustering Models")
  print("Target: Predictions, Recommendations and Presenting Data\n")
  print("Project Made by Tal Golan\n")
  

def PrintSample():
  global df5
  print("------------------------------",bcolors.CYAN,"Data Sample",bcolors.RESET,\
        "------------------------------------\n")
  print(df5.sample(10).to_markdown(),"\n")

def PrintIndexes(Model_Type):
  global MLRs,NNs
  if Model_Type=='MLR':
      print("There Are ",bcolors.CYAN,"%d "%(len(MLRs)),bcolors.RESET,"Regressions already done:\n" )
      for key in MLRs:
        print("Index:",bcolors.GREEN,"%s"% key ,bcolors.RESET,"Site Name: %s [Pollution Prediction: %s][Prediction By: %s][By Years: %s][Date Created: %s]\n" %(MLRs[key][0][0],MLRs[key][0][1],MLRs[key][0][2],MLRs[key][0][4],MLRs[key][0][3]))
  if Model_Type=='NN':
    print("---------------------------------------")
    print("There Are",bcolors.CYAN," %d" %(len(NNs)) ,bcolors.RESET," Neural Networks already done:\n" )
    for key in NNs:
      print("Index:",bcolors.GREEN,"%s" %key ,bcolors.RESET,"Site Name: %s [Pollution Prediction: %s][Prediction By: %s][By Years: %s][Date Created: %s]\n" %(NNs[key][0][0],NNs[key][0][1],NNs[key][0][2],NNs[key][0][4],NNs[key][0][3]))

def PrintSites():
  global df5
  print("\n--------------Sites--------------\n")
  Names=list(df5['Site_Name'].unique())
  for i in range(0,len(Names)):
    print("["+Names[i]+"]",end =" ")
    if(i%10==0 and i!=0):
      print("\n")
  print("\n---------------------------------\n")

def AskSiteName():
  global df5
  while True:
    PrintSites()
    old_stdout = sys.stdout
    print(">>>>>>>>>>>>     <<<<<<<<<<<<\n")
    print(bcolors.GREEN, "<Site Name>",bcolors.RESET,": Build the model based on this site"\
          ,bcolors.GREEN,"\n  B",bcolors.RESET,": back\n")
    print(bcolors.GREEN,"\n Enter Command:",bcolors.RESET)
    Entered= input(" ")
    sys.stdout = old_stdout
    if Entered in['b','B']:
      return 'Back'
      
    if Entered in list(df5['Site_Name'].unique()):
      clear_output()
      return Entered

    print("-----------------Wrong Entry!-----------------")

def Print_Years(Name):
  global df5,All_Years
  df6=df5.copy()
  df6=df6.loc[df5['Site_Name']==Name]
  All_Years=list(df6['Year'].unique())
  
  print("\n---------------Years By Entry-----------------\n")
  for i in range(0,len(All_Years)):
    print("["+str(All_Years[i])+"]",end =" ")
    if(i%10==0 and i!=0):
      print("\n")
  print("\n----------------------------------------------\n")

def AskYears(Name):
    global df6,All_Years
    c=1
    while True:  
      Print_Years(Name)
      print(bcolors.GREEN, "<Year>",bcolors.RESET,": Build the model based on year"\
            ,bcolors.GREEN,"\n  A",bcolors.RESET,": All\n")
      if c==0:
        print("----------Wrong Entry!----------")
        c=1
      Entered= list(map(str,input("Enter: (Seperate with space to enter multiple years)\n").split()))
      for item in Entered:
        if item in ['a','A']:
         return All_Years
      All_Years2 = ''.join(str(x) for x in All_Years)
      for item in Entered: 
        if item not in All_Years2:
          c=0
          break

      if c==0:
        clear_output()
        continue
      Entered2 = [ int(x) for x in Entered ]
      return Entered2

"""## Interface Functions

### Regression
"""

# Commented out IPython magic to ensure Python compatibility.
def StartR():
  global MLRs,NameEntered
  c=0
  while True:
    clear_output()
    PrintIndexes("MLR")
    print(bcolors.GREEN,"\n   Y",bcolors.RESET,": Starting New Linear Regression\n",\
                    bcolors.GREEN,"<Index>",bcolors.RESET,": Use Existing Linear Regression\n"\
                    ,bcolors.GREEN," B",bcolors.RESET,\
                    ": Back\n")
    if c!=0:
      print("-----------------Wrong Entry!-----------------")
    print(bcolors.GREEN,"\n Enter Command:",bcolors.RESET)
    Entered= input(" ")
    clear_output()
    if Entered in ['Y','y']:
      Name=AskSiteName()
      NameEntered=Name
      if Name=='Back':
        return 'Back'
      choice_input("MLR",Name)
      return 'R'
    if Entered in MLRs.keys():
      print("\n\n >>>>>>>>>>>>>>>>>>>>>>>>  Linear Regression Details  <<<<<<<<<<<<<<<<<<<<<<<< \n\nSite: %s | Prediction By: %s | Pollution Variables: %s| By Years: %s\n" \
#             %(MLRs[Entered][0][0],MLRs[Entered][0][2],MLRs[Entered][0][1],MLRs[Entered][0][4] )) 
      replace(Entered,MLRs,"MLR")
      use("MLR") 
      NameEntered=MLRs[Entered][0][0]
      return 'R'
    if Entered in['b','B']:
      return 'Back'
    c+=1
    if c==10:
      return 'Back'

"""###Neural Network"""

# Commented out IPython magic to ensure Python compatibility.
def StartN():
  global NNs,NameEntered
  c=0
  while True:
    clear_output()
    PrintIndexes("NN")
    print(bcolors.GREEN,"\n   Y",bcolors.RESET,": Starting New Neural Network\n",\
                    bcolors.GREEN,"<Index>",bcolors.RESET,": Use Existing Neural Network\n"\
                    ,bcolors.GREEN," B",bcolors.RESET," Back")
    if c!=0:
      print("-----------------Wrong Entry!-----------------")
    print(bcolors.GREEN,"\n Enter Command:",bcolors.RESET)
    Entered= input(" ")
    clear_output()
    if Entered in ['Y','y']:
      Name=AskSiteName()
      NameEntered=Name
      if Name=='Back':
        return 'Back'
      choice_input("NN",Name)
      return 'N'

    if Entered in NNs.keys():
      
      print("\n\n >>>>>>>>>>>>>>>>>>>>>>>>  Neural Network Details  <<<<<<<<<<<<<<<<<<<<<<<< \n\nSite: %s | Prediction By: %s | Pollution Variables: %s\n | Years: %s\n" \
#             %(NNs[Entered][0][0],NNs[Entered][0][2],NNs[Entered][0][1],NNs[Entered][0][4])) 
      replace(Entered,NNs,"NN")
      use("NN") 
      NameEntered=NNs[Entered][0][0]
      return 'N'
    if Entered in['b','B']:
      return 'Back'
    c+=1
    if c==10:
      return 'Back'

"""### Prediction Choice"""

def choice_input(Model_Type,Name):
  c=0
  while True:
    print("Which feature to predict? ")
    print(bcolors.GREEN,"\n  1",bcolors.RESET,": Total\n",\
                    bcolors.GREEN,"2",bcolors.RESET,": Local\n"\
                    ,bcolors.GREEN,"3",bcolors.RESET,\
                    ": Tourists\n")
    if c==1:
      print("--------Wrong Entry!-----------\n")
      
    Predict1= input(" ")
    clear_output()

    if Predict1 == '1':
      Predict='Total'
      break

    if Predict1 == '2':
      Predict='Israelis_Count'
      break

    if Predict1 == '3':
      Predict='Tourists_Count'
      break

    clear_output()
    c=1
    
  print("")
  c=0
  years=AskYears(Name)
  clear_output()
  while True:
    
    print("Predict With Pollution Variables?  "\
                    ,bcolors.GREEN,"Y/N",bcolors.RESET)
    print(bcolors.GREEN,"\n Enter Command:",bcolors.RESET)
    if c==1:
      print("--------Wrong Entry!-----------\n")
      
    Pol= input(" ")
    clear_output()

    if Pol in ['Y','y']:
      Pol=True
      break
    if Pol in ['N','n']:
      Pol=False
      break
    c=1
    
  clear_output()
  if Model_Type=="MLR":
    print("\n\n >>>>>>>>>>>>  MLR Details  <<<<<<<<<<<< \n\nSite: %s | Prediction By: %s | Pollution Variables: %s| Years: %s\n" %(Name,Predict,Pol,years))
    StartAndSave(Name,Pol,Predict,years,"MLR")
  if Model_Type=="NN":
    print("\n\n >>>>>>>>>>>>  NN Details  <<<<<<<<<<<< \n\nSite: %s | Prediction By: %s | Pollution Variables: %s| Years: %s\n" %(Name,Predict,Pol,years))
    StartAndSave(Name,Pol,Predict,years,"NN")

"""### Nature Reserves Interface num.1"""

def Go_Back():
  count=0
  print(bcolors.GREEN,"\n B",bcolors.RESET,":  Back")
  while True:
    print(bcolors.GREEN,"Enter Command:",bcolors.RESET)
    inpt= input(" ")
    if inpt in['B','b']:
      return 'back'
    if count==10:
      return 'back'
    count+=1
    print("Wrong Entry")

def After_Clustering():
  global Clusters_List
  count=0
  c=0
  while True:
    
    print(bcolors.GREEN," K",bcolors.RESET,":  Change Clustering K-Means\n"\
        ,bcolors.GREEN,"F",bcolors.RESET,":  Find The Optimal K-Means\n"\
        ,bcolors.GREEN,"B",bcolors.RESET,":  Back\n")
    
    if c==1:
      print("---------Wrong Entry--------")
      c=0
    print(bcolors.GREEN,"\n Enter Command:",bcolors.RESET)
    inpt= input(" ")
    clear_output()
    if inpt in['B','b']:
      return 'Back'
    if count==10:
      return 'Back'
    if inpt in['K','k']:
      count2=0
      c2=0
      while True:
        print(" Enter" ,bcolors.GREEN,"<K-means>" ,bcolors.RESET,":  Build A New Clustering Model"\
              "\n Enter",bcolors.GREEN,"B",bcolors.RESET,":  Back" )
        print(bcolors.GREEN,"\n Enter Command:",bcolors.RESET)

        if c2==1:
          print("--------Wrong Entry!------------")
          c2=0

        NewK= input(" ")
        clear_output()
        if NewK in ['b','B']:
          return 'Back'
        try:
          NewK= int(NewK)
          MaxK=0
          for y in [len(x) for x in Clusters_List]:
            MaxK+=y
          
          if NewK>1 and NewK<MaxK:
            ClusterChooseK(NewK)
            print("K-means Updated")
            return 'Updated'
        except Exception as e: 
          print(e)
          c2=2
          count2+=1
          continue
        c2=1
        count2+=1
        if count2==10:
          return 'Back'
    if inpt in['F','f']:
      K_Evaluation()
      Sta= input("Enter ANY key to continue ")
      return 'Back'


    count+=1
    c=1
  

def Start_Menu():
  global NameEntered
  c=0
  count=0
  while True:
    if c !=0:
      clear_output()
    Hello()
    print("------------------------------",bcolors.CYAN,"Models",bcolors.RESET,"------------------------------------\n")

    print(bcolors.GREEN," R",bcolors.RESET,":  Linear Regression Prediction\n",\
                    bcolors.GREEN,"N",bcolors.RESET,":  Neural Network Prediction\n"\
                    ,bcolors.GREEN, "C",bcolors.RESET,":  Clustering Information \n",\
                    bcolors.GREEN, "S",bcolors.RESET,":  Site Information and Recommendations\n"\
          

                    "--------------------------------",bcolors.CYAN,"Project",bcolors.RESET\
                    ,"----------------------------------\n"\
          
                    ,bcolors.GREEN,"Save",bcolors.RESET,":  Save Models\n"\
                    ,bcolors.GREEN,"Upload",bcolors.RESET,":  Upload Models (will override current status)\n"\
                    ,bcolors.GREEN,"Sample",bcolors.RESET,":  Data Sample\n"\
                    ,bcolors.GREEN,"Reset",bcolors.RESET,":  Reset Project (Models Will Be Lost)\n"\
                   ,bcolors.GREEN,"Exit",bcolors.RESET,":  Exit\n")
    

    if c==1:
      print("\n--------------",bcolors.CYAN,"Wrong Entry!",bcolors.RESET,"--------------")
      
    if c==3:
      print("\n---------------",bcolors.CYAN,"K-Means Updated!",bcolors.RESET,"----------------------")

    if c==5:
      print("\n---------------",bcolors.CYAN,"Models Saved!",bcolors.RESET,"------------------------")
      print("\n--",bcolors.CYAN,"(You Can Download The Save File From Colab)",bcolors.RESET,"-------")

    if c==6:
      print("\n---------------",bcolors.CYAN,"Models Uploaded!",bcolors.RESET,"------------------------")

    if c==7:
      print("\n---------------",bcolors.CYAN,"Error Uploading file: batch file/canceled upload",bcolors.RESET,"--------------------")

    if c==8:
      print("\n---------------",bcolors.CYAN,"Project Has Reset Succesfully",bcolors.RESET,"------------------------")
      
    c=0
    if count==10:
      return 'Back'
    print(bcolors.GREEN," Enter Command:",bcolors.RESET)
    Start= input("  ")
    if Start in['Exit','EXIT','exit']:
      return "Exit"
    clear_output()
    Ccheck=0
    if Start in['R','r']:
      return StartR()
    
    if Start in['c','C']:
      P_Clusters()
      After_Cluster=After_Clustering()
      if After_Cluster =='Back':
        c='clear'
        continue
      if After_Cluster=='Updated':
        c=3
        continue
      
    if Start in['n','N']:
      return StartN()
    if Start in['s','S']:
      
      Entered_Site=AskSiteName()
      NameEntered=Entered_Site
      if Entered_Site =='Back':
        c='back'
        continue
      Site_Info(Entered_Site)
      output_info=0
      while True:
        print(bcolors.GREEN,"\n  B",bcolors.RESET,":  Go Back\n"\
              ,bcolors.GREEN,"M",bcolors.RESET,":  Get Site info by date\n")
        if output_info==1:
          print("-----------wrong entry!-----------")
        print("\n",bcolors.GREEN,"Enter Command:",bcolors.RESET)
        After_Info= input("")
        clear_output()
        if After_Info in['B','b']:
          Ccheck=1
          break
        if After_Info in['M','m']:
          Enter_Date(Entered_Site)
          continue

        output_info=1
      
      if Ccheck==1:
        c=4
        Ccheck=0
        continue
    
    if Start in ['SAVE','save',"Save"]:
      Save_All()
      c=5
      continue
    if Start in ['Upload','UPLOAD',"upload"]:
      if Upload_All() is False:
        c=7
        continue
      c=6
      continue
    if Start in ['Sample','SAMPLE','sample']:
      PrintSample()
      Go_Back()
      c='clear'
      continue
    if Start in ['RESET','reset','Reset']:
      global MLRs,NNs,Clustering
      MLRs={}
      NNs={}
      c=8
      ClusterChooseK(20)
      continue

    c=1
    count+=1

"""### Nature Reserves Interface num.2"""

#prep functions run
def DataPrep_Nature_Reserves():
  global Run_Check
  if Run_Check==0:
    Main_Project()
    Starting_Data_Prep_Main()
    choice()
    Date_Data_prep()
  Run_Check=1
# Main of Nature Reserves
def Execute():
  DataPrep_Nature_Reserves()
  while True:
    Answer=None
    global K_means,NameEntered,Clusters_List,Clustering
    
    try:
      if not Clustering:
        ClusterChooseK(20)
      Answer=Start_Menu()
      if Answer=='Exit':
        print("\n------------Closed. Run again to continue.------------")
        break
      if Answer=='Back':
        clear_output()
        continue
    

      def MenuChoice():
        
        
        print(">>>>>>>>>>>>      Prediction     <<<<<<<<<<<<\n")
        print(bcolors.GREEN,"  Y",bcolors.RESET,": Predict",bcolors.GREEN,"X",\
                  bcolors.RESET,": Stop",bcolors.GREEN,"B",bcolors.RESET,": Back\n")
        
        while(True):
          print(bcolors.GREEN,"\n Enter Command:",bcolors.RESET)
          Sta= input(" ")
          count=1
          if Sta in['b','B'] or count==10:
            clear_output()
            return 5
          if Sta in['x','X']:
            return False
          if Sta in['y','Y']:
            return True
          print("--------------Wrong Input!--------------")
          count+=1

      inputcheck=0
      while inputcheck==0:
        Nr=MenuChoice()
        if Nr == 5:
          inputcheck=1
          break
        if Nr == False:
          inputcheck=0
          break
        if Nr== True:
          if Answer=='R':
            PredictX(PredInput(),"MLR")
          if Answer=='N':
            PredictX(PredInput(),"NN")

      if inputcheck==1:
        continue
      if inputcheck==0:
        print("\n------------Closed. Run again to continue.------------")
        break
    except Exception as e:
      clear_output()
      print(e)
      try:
        Sta= input("Enter ANY key to continue ")
        clear_output()
        continue
      except KeyboardInterrupt:
        print(bcolors.YELLOW,'Interrupted By Input',bcolors.RESET)
        return
      
    except KeyboardInterrupt:
      print(bcolors.YELLOW,'Interrupted By Input',bcolors.RESET)
      return

"""# Execute

##Main Project Execute
"""

Execute()